UID,title,authors,abstract,keywords,track,paper_type,pdf_url,demo_url,material
demo.102,"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",Ian Tenney|James Wexler|Jasmijn Bastings|Tolga Bolukbasi|Andy Coenen|Sebastian Gehrmann|Ellen Jiang|Mahima Pushkarna|Carey Radebaugh|Emily Reif|Ann Yuan,"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models---including classification, seq2seq, and structured prediction---and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.",visualization models|counterfactual generation|rapid exploration|rapid analysis,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.15,,[Language Interpretability Tool website](https://pair-code.github.io/lit)
demo.104,"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",John Morris|Eli Lifland|Jin Yong Yoo|Jake Grigsby|Di Jin|Yanjun Qi,"While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16  adversarial attacks  from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",adversarial attacks|data augmentation|adversarial nlp|glue tasks,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.16,,
demo.107,"Easy, Reproducible and Quality-Controlled Data Collection with CROWDAQ",Qiang Ning|Hao Wu|Pradeep Dasigi|Dheeru Dua|Matt Gardner|Robert L Logan IV|Ana Marasović|Zhen Nie,"High-quality and large-scale data are key to success for AI systems. However, large-scale data annotation efforts are often confronted with a set of common challenges: (1) designing a user-friendly annotation interface; (2) training enough annotators efficiently; and (3) reproducibility. To address these problems, we introduce CROWDAQ, an open-source platform that standardizes the data collection pipeline with customizable user-interface components, automated annotator qualification, and saved pipelines in a re-usable format. We show that CROWDAQ simplifies data annotation significantly on a diverse set of data collection use cases and we hope it will be a convenient tool for the community.",ai systems|automated qualification|data annotation|crowdaq,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.17,,
demo.109,SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search,Tom Hope|Jason Portenoy|Kishore Vasan|Jonathan Borchardt|Eric Horvitz|Daniel Weld|Marti Hearst|Jevin West,"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery of connections across a corpus. In this paper, we present SciSight, a system for exploratory search of COVID-19 research integrating two key capabilities: first, exploring associations between biomedical facets automatically extracted from papers (e.g., genes, drugs, diseases, patient outcomes); second, combining textual and network information to search and visualize groups of researchers and their ties. SciSight has so far served over 15K users with over 42K page views and 13\% returns.",targeted queries|discovery connections|exploratory research|scisight,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.18,,
demo.111,SIMULEVAL: An Evaluation Toolkit for Simultaneous Translation,Xutai Ma|Mohammad Javad Dousti|Changhan Wang|Jiatao Gu|Juan Pino,"Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.",simultaneous translation|real-time scenario|evaluating models|simultaneous scenario,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.19,,
demo.116,Agent Assist through Conversation Analysis,Kshitij Fadnis|Nathaniel Mills|Jatin Ganhotra|Haggai Roitman|Gaurav Pandey|Doron Cohen|Yosi Mass|Shai Erera|Chulaka Gunasekara|Danish Contractor|Siva Patel|Q. Vera Liao|Sachindra Joshi|Luis Lastras|David Konopnicki,"Customer support agents play a crucial role as an interface between an organization and its end-users. We propose CAIRAA: Conversational Approach to Information Retrieval for Agent Assistance, to reduce the cognitive workload of support agents who engage with users through conversation systems. CAIRAA monitors an evolving conversation and recommends both responses and URLs of documents the agent can use in replies to their client. We combine traditional information retrieval (IR) approaches with more recent Deep Learning (DL) models to ensure high accuracy and efficient run-time performance in the deployed system. Here, we describe the CAIRAA system and demonstrate its effectiveness in a pilot study via a short video.",information retrieval|agent assistance|run-time|customer agents,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.20,,
demo.118,NeuSpell: A Neural Spelling Correction Toolkit,Sai Muralidhar Jayanthi|Danish Pruthi|Graham Neubig,"We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To remedy this, (i) we train neural models using spelling errors in context, synthetically constructed by reverse engineering isolated misspellings; and (ii) use richer representations of the context. By training on our synthetic examples, correction rates improve by 9% (absolute) compared to the case when models are trained on randomly sampled character perturbations. Using richer contextual representations boosts the correction rate by another 3%. Our toolkit enables practitioners to use our proposed and existing spelling correction systems, both via a simple unified command line, as well as a web interface. Among many potential applications, we demonstrate the utility of our spell-checkers in combating adversarial misspellings. The toolkit can be accessed at neuspell.github.io.",spelling correction|spell-checkers|neuspell|open-source toolkit,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.21,,
demo.119,LibKGE - A knowledge graph embedding library for reproducible research,Samuel Broscheit|Daniel Ruffinelli|Adrian Kochsiek|Patrick Betz|Rainer Gemulla,"LibKGE ( https://github.com/uma-pi1/kge )  is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LibKGE are to enable reproducible research, to provide a framework for comprehensive experimental studies, and to facilitate analyzing the contributions of individual components of training methods, model architectures, and evaluation methods. LibKGE is highly configurable and every experiment can be fully reproduced with a single configuration file. Individual components are decoupled to the extent possible so that they can be mixed and matched with each other. Implementations in LibKGE aim to be as efficient as possible without leaving the scope of Python/Numpy/PyTorch. A comprehensive logging mechanism and tooling facilitates in-depth analysis. LibKGE provides implementations of common knowledge graph embedding models and training methods, and new ones can be easily added. A comparative study (Ruffinelli et al., 2020) showed that LibKGE reaches competitive to state-of-the-art performance for many models with a modest amount of automatic hyperparameter tuning.",training|hyperparameter optimization|link prediction|in-depth analysis,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.22,,
demo.123,WantWords: An Open-source Online Reverse Dictionary System,Fanchao Qi|Lei Zhang|Yanhui Yang|Zhiyuan Liu|Maosong Sun,"A reverse dictionary takes descriptions of words as input and outputs words semantically matching the input descriptions. Reverse dictionaries have great practical value such as solving the tip-of-the-tongue problem and helping new language learners. There have been some online reverse dictionary systems, but they support English reverse dictionary queries only and their performance is far from perfect. In this paper, we present a new open-source online reverse dictionary system named WantWords (https://wantwords.thunlp.org/). It not only significantly outperforms other reverse dictionary systems on English reverse dictionary performance, but also supports Chinese and English-Chinese as well as Chinese-English cross-lingual reverse dictionary queries for the first time. Moreover, it has user-friendly front-end design which can help users find the words they need quickly and easily. All the code and data are available at https://github.com/thunlp/WantWords.",english dictionary|reverse dictionaries|language learners|online systems,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.23,,[Online System](https://wantwords.thunlp.org/)
demo.124,BENNERD: A Neural Named Entity Linking System for COVID-19,Mohammad Golam Sohrab|Khoa Duong|Makoto Miwa|Goran Topic|Ikeda Masami|Takamura Hiroya,"We present a biomedical entity linking (EL) system BENNERD that detects named enti- ties in text and links them to the unified medical language system (UMLS) knowledge base (KB) entries to facilitate the corona virus disease 2019 (COVID-19) research. BEN- NERD mainly covers biomedical domain, es- pecially new entity types (e.g., coronavirus, vi- ral proteins, immune responses) by address- ing CORD-NER dataset. It includes several NLP tools to process biomedical texts includ- ing tokenization, flat and nested entity recog- nition, and candidate generation and rank- ing for EL that have been pre-trained using the CORD-NER corpus. To the best of our knowledge, this is the first attempt that ad- dresses NER and EL on COVID-19-related entities, such as COVID-19 virus, potential vaccines, and spreading mechanism, that may benefit research on COVID-19. We release an online system to enable real-time entity annotation with linking for end users. We also release the manually annotated test set and CORD-NERD dataset for leveraging EL task. The BENNERD system is available at https://aistairc.github.io/BENNERD/.",corona research|flat recog-|flat nition|candidate generation,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.24,,[BENNERD web page](https://aistairc.github.io/BENNERD/)
demo.126,RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text,Liam Dugan|Daphne Ippolito|Arun Kirubarajan|Chris Callison-Burch,"In recent years, large neural networks for natural language generation (NLG) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between NLG systems and understanding how humans perceive the generated text remain both crucial and difficult. In this system demonstration, we present Real or Fake Text (RoFT), a website that tackles both of these challenges by inviting users to try their hand at detecting machine-generated text in a variety of domains. We introduce a novel evaluation task based on detecting the boundary at which a text passage that starts off human-written transitions to being machine-generated. We show preliminary results of using RoFT to evaluate detection of machine-generated news articles.",natural generation|evaluating differences|detecting text|detection articles,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.25,,
demo.127,A Data-Centric Framework for Composable NLP Workflows,Zhengzhong Liu|Guanxiong Ding|Avinash Bukkittu|Mansi Gupta|Pengzhi Gao|Atif Ahmed|Shikun Zhang|Xin Gao|Swapnil Singhavi|Linwei Li|Wei Wei|Zecong Hu|Haoran Shi|Xiaodan Liang|Teruko Mitamura|Eric Xing|Zhiting Hu,"Empirical natural language processing (NLP) systems in application domains (e.g., healthcare, finance, education) involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, analysis, generation, and visualization. We establish a unified open-source framework to support fast development of such sophisticated NLP workflows in a composable manner. The framework introduces a uniform data representation to encode heterogeneous results by a wide range of NLP tasks. It offers a large repository of processors for NLP tasks, visualization, and annotation, which can be easily assembled with full interoperability under the unified representation. The highly extensible framework allows plugging in custom processors from external off-the-shelf NLP and deep learning libraries. The whole framework is delivered through two modularized yet integratable open-source projects, namely Forte (for workflow infrastructure and NLP function processors) and Stave (for user interaction, visualization, and annotation).",data ingestion|human annotation|text retrieval|analysis,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.26,,
demo.128,CoRefi: A Crowd Sourcing Suite for Coreference Annotation,Ari Bornstein|Arie Cattan|Ido Dagan,"Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present CoRefi, a web-based coreference annotation suite, oriented for crowdsourcing. Beyond the core coreference annotation tool, CoRefi provides guided onboarding for the task as well as a novel algorithm for a reviewing phase. CoRefi is open source and directly embeds into any website, including popular crowdsourcing platforms.  CoRefi Demo: aka.ms/corefi Video Tour: aka.ms/corefivideo Github Repo: https://github.com/aribornstein/corefi",coreference annotation|annotation|crowdsourcing|reviewing phase,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.27,,
demo.131,Langsmith: An Interactive Academic Text Revision System,Takumi Ito|Tatsuki Kuribayashi|Masatoshi Hidaka|Jun Suzuki|Kentaro Inui,"Despite the current diversity and inclusion initiatives in the academic community, researchers with a non-native command of English still face signiﬁcant obstacles when writing papers in English. This paper presents the Langsmith editor, which assists inexperienced, non-native researchers to write English papers, especially in the natural language processing (NLP) ﬁeld. Our system can suggest ﬂuent, academic-style sentences to writers based on their rough, incomplete phrases or sentences. The system also encourages interaction between human writers and the computerized revision system. The experimental results demonstrated that Langsmith helps non-native English-speaker students write papers in English. The system is available at https://emnlp-demo.editor. langsmith.co.jp/.",natural eld|langsmith editor|human writers|computerized system,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.28,,
demo.132,IsOBS: An Information System for Oracle Bone Script,Xu Han|Yuzhuo Bai|Keyue Qiu|Zhiyuan Liu|Maosong Sun,"Oracle bone script (OBS) is the earliest known ancient Chinese writing system and the ancestor of modern Chinese. As the Chinese writing system is the oldest continuously-used system in the world, the study of OBS plays an important role in both linguistic and historical research. In order to utilize advanced machine learning methods to automatically process OBS, we construct an information system for OBS (IsOBS) to symbolize, serialize, and store OBS data at the character-level, based on efficient databases and retrieval modules. Moreover, we also apply few-shot learning methods to build an effective OBS character recognition module, which can recognize a large number of OBS characters (especially those characters with a handful of examples) and make the system easy to use. The demo system of IsOBS can be found from \url{http://isobs.thunlp.org/}. In the future, we will add more OBS data to the system, and hopefully our IsOBS can support further efforts in automatically processing OBS and advance the scientific progress in this field.",linguistic research|obs|chinese system|machine methods,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.29,,
demo.48,OpenUE: An Open Toolkit of Universal Extraction from Text,Ningyu Zhang|Shumin Deng|Zhen Bi|Haiyang Yu|Jiacheng Yang|Mosha Chen|Fei Huang|Wei Zhang|Huajun Chen,"Natural language processing covers a wide variety of tasks with token-level or sentence-level understandings.  In this paper, we provide a simple insight that most tasks can be represented in a single universal extraction format. We introduce a prototype model and provide an open-source and extensible toolkit called OpenUE for various extraction tasks. OpenUE allows developers to train custom models to extract information from the text and supports quick model validation for researchers. Besides, OpenUE provides various functional modules to maintain sufficient modularity and extensibility. Except for the toolkit, we also deploy an online demo with restful APIs to support real-time extraction without training and deploying. Additionally, the online system can extract information in various tasks, including relational triple extraction, slot & intent detection, event extraction, and so on. We release the source code, datasets, and pre-trained models to promote future researches in http://github.com/zjunlp/openue.",natural processing|extraction tasks|real-time extraction|relational extraction,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.1,,
demo.49,BERTweet: A pre-trained language model for English Tweets,Dat Quoc Nguyen|Thanh Vu|Anh Tuan Nguyen,"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet",tweet tasks|part-of-speech tagging|named-entity recognition|text classification,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.2,,[BERTweet](https://github.com/VinAIResearch/BERTweet)
demo.54,NeuralQA: A Usable Library for Question Answering (Contextual Query Expansion + BERT) on Large Datasets,Victor Dibia,"Existing tools for Question Answering (QA) have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline (query expansion, retrieval, reading, and explanation/sensemaking). To help address these issues, we introduce NeuralQA - a usable library for QA on large datasets. NeuralQA integrates well with existing infrastructure (e.g.,  ElasticSearch instances and reader models trained with the HuggingFace Transformers API) and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion (CQE) using a masked language model (MLM) as well as relevant snippets (\(RelSnip\)) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations (e.g., visualization of gradient-based explanations to support qualitative inspection of model behaviour) and large scale search deployment. Code and documentation for NeuralQA is available as open source on  \href{https://github.com/victordibia/neuralqa}{Github}.",question qa|question|query expansion|retrieval,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.3,,[Project Code Repository](https://github.com/victordibia/neuralqa)|[Pre-recorded Screencast](https://vimeo.com/472721886)
demo.58,Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia,Ikuya Yamada|Akari Asai|Jin Sakuma|Hiroyuki Shindo|Hideaki Takeda|Yoshiyasu Takefuji|Yuji Matsumoto,"The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Python-based open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also introduce a web-based demonstration of our tool that allows users to visualize and explore the learned embeddings. In our experiments, our tool achieved a state-of-the-art result on the KORE entity relatedness dataset, and competitive results on various standard benchmark datasets. Furthermore, our tool has been used as a key component in various recent studies. We publicize the source code, demonstration, and the pretrained embeddings for 12 languages at https://wikipedia2vec.github.io/.",natural tasks|wikipediavec|python-based tool|embeddings entities,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.4,,
demo.59,ARES: A Reading Comprehension Ensembling Service,Anthony Ferritto|Lin Pan|Rishav Chakravarti|Salim Roukos|Radu Florian|J William Murdock|Avi Sil,"We introduce ARES (A Reading Comprehension  Ensembling  Service):   a  novel  Machine Reading  Comprehension  (MRC)  demonstration  system  which  utilizes  an  ensemble  of models  to  increase  F1  by  2.3  points.   While many  of  the  top  leaderboard  submissions  in popular  MRC  benchmarks  such  as  the  Stanford Question  Answering  Dataset  (SQuAD) and Natural Questions (NQ) use model ensembles, the accompanying papers do not publish their ensembling strategies.  In this work, we detail and evaluate various ensembling strategies using the NQ dataset.   ARES  leverages the CFO (Chakravarti et al., 2019) and ReactJS  distributed  frameworks  to  provide  a  scalable  interactive  Question  Answering  experience that capitalizes on the agreement (or lack thereof)  between  models  to  improve  the  answer visualization experience.",reading service|service|machine|interactive,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.5,,
demo.60,Transformers: State-of-the-Art Natural Language Processing,Thomas Wolf|Julien Chaumond|Lysandre Debut|Victor Sanh|Clement Delangue|Anthony Moi|Pierric Cistac|Morgan Funtowicz|Joe Davison|Sam Shleifer|Remi Louf|Patrick von Platen|Tim Rault|Yacine Jernite|Teven Le Scao|Sylvain Gugger|Julien Plu|Clara Ma|Canwei Shen|Mariama Drame|Quentin Lhoest|Alexander Rush,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.  The library is available at https://github.com/huggingface/transformers.",natural processing|model pretraining|model architecture|transformer architectures,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.6,,
demo.71,AdapterHub: A Framework for Adapting Transformers,Jonas Pfeiffer|Andreas Rücklé|Clifton Poth|Aishwarya Kamath|Ivan Vulić|Sebastian Ruder|Kyunghyun Cho|Iryna Gurevych,"The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters---small learnt bottleneck layers inserted within each layer of a pre-trained model--- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic ""stiching-in"" of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml",modus operandi|downloading models|downloading|nlp methods,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.7,,
demo.72,HUMAN: Hierarchical Universal Modular ANnotator,Moritz Wolf|Dana Ruiter|Ashwin Geet D'Sa|Liane Reiners|Jan Alexandersson|Dietrich Klakow,"A lot of real-world phenomena are complex and cannot be captured by single task annotations. This causes a need for subsequent annotations, with interdependent questions and answers describing the nature of the subject at hand. Even in the case a phenomenon is easily captured by a single task, the high specialisation of most annotation tools can result in having to switch to another tool if the task only slightly changes.  We introduce HUMAN, a novel web-based annotation tool that addresses the above problems by a) covering a variety of annotation tasks on both textual and image data, and b) the usage of an internal deterministic state machine, allowing the researcher to chain different annotation tasks in an interdependent manner. Further, the modular nature of the tool makes it easy to define new annotation tasks and integrate machine learning algorithms e.g., for active learning. HUMAN comes with an easy-to-use graphical user interface that simplifies the annotation task and management.",annotation tasks|active learning|annotation management|annotation tools,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.8,,
demo.79,DeezyMatch: A Flexible Deep Learning Approach to Fuzzy String Matching,Kasra Hosseini|Federico Nanni|Mariona Coll Ardanuy,"We present DeezyMatch, a free, open-source software library written in Python for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a pretrained model, which paves the way for transfer learning in fuzzy string matching. This approach is especially useful where only limited training examples are available. The learned DeezyMatch models can be used to generate rich vector representations from string inputs. The candidate ranker component in DeezyMatch uses these vector representations to find, for a given query, the best matching candidates in a knowledge base. It uses an adaptive searching algorithm applicable to large knowledge bases and query sets. We describe DeezyMatch's functionality, design and implementation, accompanied by a use case in toponym matching and candidate ranking in realistic noisy datasets.",fuzzy matching|candidate ranking|fine-tuning model|toponym matching,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.9,,[Project Code Repository](https://github.com/Living-with-machines/DeezyMatch)
demo.86,CoSaTa: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences,Peter Jansen,"This work presents CoSaTa, an intuitive constraint satisfaction solver and interpreted language for knowledge bases of semi-structured tables expressed as text.  The stand-alone CoSaTa solver allows easily expressing complex compositional ""inference patterns"" for how knowledge from different tables tends to connect to support inference and explanation construction in question answering and other downstream tasks, while including advanced declarative features and the ability to operate over multiple representations of text (words, lemmas, or part-of-speech tags).  CoSaTa also includes a hybrid imperative/declarative interpreted language for expressing simple models through minimally-specified simulations grounded in constraint patterns, helping bridge the gap between question answering, question explanation, and model simulation.  The solver and interpreter are released as open source. Screencast Demo: https://youtu.be/t93Acsz7LyE",inference construction|question answering|question explanation|model simulation,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.10,,
demo.89,InVeRo: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles,Simone Conia|Fabrizio Brignone|Davide Zanfardino|Roberto Navigli,"Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pretrained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use Web interface and RESTful APIs available at http://nlp.uniroma1.it/invero.",srl|neural models|neural architecture|web interface,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.11,,
demo.91,Youling: an AI-assisted Lyrics Creation System,Rongsheng Zhang|Xiaoxi Mao|Le Li|Lin Jiang|Lin Chen|Zhiwei Hu|Yadong Xi|Changjie Fan|Minlie Huang,"Recently, a variety of neural models have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little human intervention. We believe that lyrics creation is a creative process with human intelligence centered. AI should play a role as an assistant in the lyrics creation process, where human interactions are crucial for high-quality creation. This paper demonstrates \textit{Youling}, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, \textit{Youling} supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The system also provides a revision module which enables users to revise undesired sentences or words of lyrics repeatedly. Besides, \textit{Youling} allows users to use multifaceted attributes to control the content and format of generated lyrics. The demo video of the system is available at \href{https://youtu.be/DFeNpHk0pm4}{https://youtu.be/DFeNpHk0pm4}.",lyrics generation|generation process|lyrics creation|lyrics process,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.12,,[Pre-recorded screencast](https://youtu.be/DFeNpHk0pm4)|[Link to our system](https://yl.fuxi.netease.com/) . Visitors can log in with the public account (youlingtest@163.com) and password (youling666)
demo.93,A Technical Question Answering System with Transfer Learning,Wenhao Yu|Lingfei Wu|Yu Deng|Ruchi Mahindru|Qingkai Zeng|Sinem Guven|Meng Jiang,"In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving proper answers based on correctly answered similar questions in the past. TransTQA is built upon a siamese ALBERT network, which enables it to respond quickly and accurately. Furthermore, TransTQA adopts a standard deep transfer learning strategy to improve its capability of supporting multiple technical domains.",transtqa|siamese network|deep strategy|automatic responses,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.13,,
demo.97,ENTYFI: A System for Fine-grained Entity Typing in Fictional Texts,Cuong Xuan Chu|Simon Razniewski|Gerhard Weikum,"Fiction and fantasy are archetypes of long-tail domains that lack suitable NLP methodologies and tools. We present ENTYFI, a web-based system for fine-grained typing of entity mentions in fictional texts. It builds on 205 automatically induced high-quality type systems for popular fictional domains, and provides recommendations towards reference type systems for given input texts. Users can exploit the richness and diversity of these reference type systems for fine-grained supervised typing, in addition, they can choose among and combine four other typing modules: pre-trained real-world models, unsupervised dependency-based typing, knowledge base lookups, and constraint-based candidate consolidation. The demonstrator is available at: https://d5demos.mpi-inf.mpg.de/entyfi.",fine-grained mentions|fine-grained typing|demonstrator|nlp methodologies,System Demonstrations,demo,https://www.aclweb.org/anthology/2020.emnlp-demos.14,,[Project page](https://www.mpi-inf.mpg.de/yago-naga/entyfi)|[Pre-recorded Screencast](https://www.youtube.com/watch?v=g_ESaONagFQ)
