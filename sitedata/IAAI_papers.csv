UID,title,authors,abstract,pdf_url,SessionInformation,presentation_id,GatherTownLink,ZoomLink
IAAI-23,Enhancing E-commerce Recommender System Adaptability with Online Deep Controllable Learning-To-Rank,Anxiang Zeng|Han Yu|Hualin He|Yabo Ni|Yongliang Li|Jingren Zhou|Chunyan Miao,"In the past decade, recommender systems for e-commerce have witnessed significant advancement. Recommendation scenarios can be divided into different type (e.g., pre-, during-, post-purchase, campaign, promotion, bundle) for different user groups or different businesses. For different scenarios, the goals of recommendation are different. This is reflected by the different performance metrics employed. In addition, online promotional campaigns, which attract high traffic volumes, are also a critical factor affecting e-commerce recommender systems. Typically, prior to a promotional campaign, the Add-to-Cart Rate (ACR) is the target of optimization. During the campaign, this changes to Gross Merchandise Volumes (GMV). Immediately after the campaign, it becomes Click Through Rates CTR. Dynamically adapting among these potentially conflicting optimization objectives is an important capability for recommender systems deployed in real-world e-commerce platforms. In this paper, we report our experience designing and deploying the Deep Controllable Learning-To-Rank (DC-LTR) recommender system to address this challenge. It enhances the feedback controller in LTR with multi-objective optimization so as to maximize different objectives under constraints. Its ability to dynamically adapt to changing business objectives has resulted in significant business advantages. Since September 2019, DC-LTR has become a core service enabling adaptive online training and real-time deployment ranking models based on changing business objectives in AliExpress and Lazada. Under both everyday use scenarios and peak load scenarios during large promotional campaigns, DC-LTR has achieved significant improvements in satisfying real-world business objectives.","i.e., a link to the preprint version of each paper (probably hosted in AAAI official site). We also need a zipped file of all the papers in order to extract an image from each paper","e.g., 10:00-11:00, February 7 ","i.e., for the link https://slideslive.com/paper.SlidesLivePresentationID",Each paper has a unique GatherTownLink,We need this information if there is a paper presentation session
IAAI-73,Neutrino^{TM}: A BlackBox Framework for Constrained Deep Learning Model Optimization,Anush Sankaran|Olivier Mastropietro|Ehsan Saboori|Yasser Idris|Davis Sawyer|Mohammadhossein Askarihemmat|Ghouthi Boukli Hacene,"Designing deep learning-based solutions is becoming a race for training deeper models with a greater number of layers.While a large-size deeper model could provide competitive accuracy, it creates a lot of logistical challenges and unreasonable resource requirements during development and deployment. This has been one of the key reasons for deep learning models not being excessively used in various production environments, especially in edge devices. There is an immediate requirement for optimizing and compressing these deep learning models, to enable on-device intelligence.In this research, we introduce a black-box framework, Neutrino^{TM} for production-ready optimization of deep learning models. The framework provides an easy mechanism for the end-users to provide constraints such as a tolerable drop in accuracy or target size of the optimized models, to guide the whole optimization pipeline. The framework is easy to include in an existing production pipeline and is available as a Python Package, supporting PyTorch and Tensorflow libraries. The optimization performance of the framework is shown across multiple benchmark datasets and popular deep learning models. Further, the framework is currently used in production and the results and testimonials from several clients are summarized.",,,,,
IAAI-84,Comparison Lift: Bandit-based Experimentation System for Online Advertising,Tong Geng|Harikesh Nair|Xiliang Lin|Jun Hao|Bin Xiang|Shurui Fan,"Comparison Lift is an experimentation-as-a-service (EaaS) application for testing online advertising audiences and creatives at JD.com. Unlike many other EaaS tools that focus primarily on fixed sample A/B testing, Comparison Lift deploys a custom bandit-based experimentation algorithm. The advantages of the bandit-based approach are two-fold. First, it aligns the randomization induced in the test with the advertiser’s goals from testing. Second, by adapting experimental design to information acquired during the test, it reduces substantially the cost of experimentation to the advertiser. Since launch in May 2019, Comparison Lift has been utilized in over 1,500 experiments. We estimate that utilization of the product has helped increase click-through rates of participating advertising campaigns by 46% on average. We estimate that the adaptive design in the product has generated 27% more clicks on average during testing compared to a fixed sample A/B design. Both suggest significant value generation and cost savings to advertisers from the product.",,,,,
IAAI-89,Robust PDF Document Conversion using Recurrent Neural Networks,Nikolaos Livathinos|Cesar Berrospi|Maksym Lysak|Viktor Kuropiatnyk|Ahmed Nassar|Andre Carvalho|Michele Dolfi|Christoph Auer|Kasper Dinkla|Peter W. J. Staar,"The number of published PDF documents in both the academic and commercial world has increased exponentially in recent decades. There is a growing need to make their rich content discoverable to information retrieval tools. Achieving high-quality semantic searches demands that a document's structural components such as title, section headers, paragraphs, (nested) lists, tables and figures (including their captions) are properly identified. Unfortunately, the PDF format is known to not conserve such structural information because it simply represents a document as a stream of low-level printing commands, in which one or more characters are placed in a bounding box with a particular styling. In this paper, we present a novel approach to document structure recovery in PDF using recurrent neural networks to process the low-level PDF data representation directly, instead of relying on a visual re-interpretation of the rendered PDF page, as has been proposed in previous literature. We demonstrate how a sequence of PDF printing commands can be used as input into a neural network and how the network can learn to classify each printing command according to its structural function in the page. This approach has three advantages: First, it can distinguish among more fine-grained labels (typically 10-20 labels as opposed to 1-5 with visual methods), which results in a more accurate and detailed document structure resolution. Second, it can take into account the text flow across pages more naturally compared to visual methods because it can concatenate the printing commands of sequential pages. Last, our proposed method needs less memory and it is computationally less expensive than visual methods. This allows us to deploy such models in production environments at a much lower cost. Through extensive architectural search in combination with advanced feature engineering, we were able to implement a model that yields a weighted average F1 score of 97% across 17 distinct structural labels. The best model we achieved is currently served in production environments on our Corpus Conversion Service (CCS), which was presented at KDD18. This  model enhances the capabilities of CCS significantly, as it eliminates the  need for human annotated label ground-truth for every unseen document layout. This proved particularly useful when applied  to a huge corpus of PDF articles related to COVID-19.",,,,,
IAAI-92,Automated Reasoning and Learning for Automated Payroll Management,Sebastijan Dumancic|Wannes Meert|Stijn Goethals|Tim Stuyckens|Jelle Huygen|Koen Denies,"While payroll management is a crucial aspect of any business venture, anticipating the future financial impact of changes to the payroll policy is a challenging task due to the complexity of tax legislature. The goal of this work is to automatically explore potential payroll policies and find the optimal set of policies that satisfies the user's needs. To achieve this goal, we overcome two major challenges. First, we translate the tax legislative knowledge into a formal representation flexible enough to support a variety of scenarios in payroll calculations. Second, the legal knowledge is further compiled into a set of constraints from which a constraint solver can find the optimal policy. Furthermore, payroll computation is performed on the individual basis which might be expensive for companies with a large number of employees. To make the optimisation more efficient, we integrate it with a machine learning model that learns from the previous optimisation runs and speeds up the optimisation engine. The results of this work have been deployed by a social insurance fund.",,,,,
IAAI-93,Tool for Automated Tax Coding of Invoices,Tarun Tater|Sampath Dechu|Neelamadhav Gantayat|Meena Guptha|Sivakumar Narayanan,"Accounts payable refer to the practice where organizations procure goods and services on credit which need to be reimbursed to the vendors in due time. Once the vendor raises an invoice, it undergoes through a plethora of prosecutorial vindictive process before the final payment. In this process, tax code determination is one of the challenging step, as it directly determines the amount payable to a vendor and also for regulatory compliance. However, it is error-prone, labor (resource) intensive, and needs regular training of the resources as it is done manually. Further, an error in the tax code determination induces penalties on the organization.Automatically arriving at a tax-code for a given product accurately and efficiently is a daunting task. To address this problem, we present an automated end-to-end system for tax code determination which can either be used as a standalone application or can be integrated into an existing invoice processing workflow. The proposed system determines the most relevant tax code for an invoice using attributes such as item description, vendor details, and product's geographical details such as shipping and delivery location. Design decisions for the proposed system are driven by high accuracy, explainability, coverage, business continuity, scalability, and optimal usage of computational resources.The system has been deployed in production for one of the major consumer goods companies for more than 6 months. It has already processed more than 22k items with an accuracy of  more than 94% and high confidence prediction accuracy of around 99.54%. Using this system, approximately 73% of all the invoices require no human intervention.",,,,,
IAAI-100,An Automated Engineering Assistant: Learning Parsers for Technical Drawings,Dries Van Daele|Wannes Meert|Nicholas Decleyre|Herman Dubois,"Manufacturing companies rely on technical drawings to develop new designs or adapt designs to customer preferences. The database of historical and novel technical drawings thus represents the knowledge that is core to their operations. With current methods, however, utilizing these drawings is mostly a manual and time consuming effort. In this work, we present a software tool that knows how to interpret various parts of the drawing and can translate this information to allow for automatic reasoning and machine  learning  on top of such a large database of technical drawings. For example, to find erroneous designs, to learn about patterns present in successful designs, etc. To achieve this, we propose a method that automatically learns a parser capable of interpreting technical drawings, using only limited expert interaction. The proposed method makes use of both neural methods and symbolic  methods.  Neural methods to interpret visual images and recognize parts of two-dimensional drawings. Symbolic methods to deal with the relational structure and understand the data encapsulated in complex tables present in the technical drawing. Furthermore, the output can be used, for example, to build a similarity based search algorithm. We showcase one deployed tool that is used to help engineers find relevant, previous designs more easily as they can now query the database using a partial design instead of through limited and tedious keyword searches. A partial design can be a part of the two-dimensional  drawing, part of a table,  part of  the  contained textual information, or combinations thereof.",,,,,
IAAI-124,Accurate and Interpretable Machine Learning for Transparent Pricing of Health Insurance Plans,Rohun Kshirsagar|Li-Yen Hsu|Matthew McLelland|Anushadevi Mohan|Wideet Shende|Nicolas Tilmans|Min Guo|Ankit Chheda|Meredith Trotter|Shonket Ray|Miguel Alvarado,"Health insurance companies cover half of the United States population through commercial employer-sponsored health plans and pay 1.2 trillion US dollars every year to cover medical expenses for their members.  The actuary and underwriter roles at a health insurance company serve to assess which risks to take on and how to price those risks to ensure profitability of the organization.  While Bayesian hierarchical models are the current standard in the industry to estimate risk, interest in machine learning as a way to improve upon these existing methods is increasing. Lumiata, a healthcare analytics company, ran a study with a large health insurance company in the United States.  We evaluated the ability of machine learning models to predict the per member per month cost of employer groups in their next renewal period, especially those groups who will cost less than 95\% of what an actuarial model predicts (groups with ""concession opportunities""). We developed a sequence of two models, an individual patient-level and an employer-group-level model, to predict the annual per member per month allowed amount for employer groups, based on a population of 14 million patients. Our models performed 20\% better than the insurance carrier's existing pricing model, and identified 84\% of the concession opportunities. This study demonstrates the application of a machine learning system to compute an accurate and fair price for health insurance products and analyzes how explainable machine learning models can exceed actuarial models' predictive accuracy while maintaining interpretability.",,,,,
IAAI-132,Mars Image Content Classification: Three Years of NASA Deployment and Recent Advances,Kiri Wagstaff|Steven Lu|Emily Dunkel|Kevin Grimes|Brandon Zhao|Jesse Cai|Shoshanna Cole|Gary Doran|Raymond Francis|Jake Lee|Lukas Mandrake,"The NASA Planetary Data System hosts millions of images acquired from the planet Mars. To help users quickly find images of interest, we have developed and deployed content-based classification and search capabilities for Mars orbital and surface images. The deployed systems are publicly accessible using the PDS Image Atlas. We describe the process of training, evaluating, calibrating, and deploying updates to two CNN classifiers for images collected by Mars missions. We also report on three years of deployment including usage statistics, lessons learned, and plans for the future.",,,,,
IAAI-140,An End-to-End Solution for Named Entity Recognition in eCommerce Search,Xiang Cheng|Mitchell Bowden|Bhushan Bhange|Priyanka Goyal|Thomas Packer|Faizan Javed,"Named entity recognition (NER) is a critical step in modern search query understanding. In the domain of eCommerce, identifying the key entities, such as brand and product type, can help a search engine retrieve relevant products and therefore offer an engaging shopping experience. Recent research shows promising results on shared benchmark NER tasks using deep learning methods, but there are still unique challenges in the industry regarding domain knowledge, training data, and model production. This paper demonstrates an end-to-end solution to address these challenges. The core of our solution is a novel model training framework ”TripleLearn” which iteratively learns from three separate training datasets, instead of one training set as is traditionally done. Using this approach, the best model lifts the F1 score from 69.5 to 93.3 on the holdout test data. In our offline experiments, TripleLearn improved the model performance compared to traditional training approaches which use a single set of training data. Moreover, in the online A/B test, we see significant improvements in user engagement and revenue conversion. The model has been live on homedepot.com for more than 9 months, boosting search conversions and revenue. Beyond our application, this TripleLearn framework, as well as the end-to-end process, is model-independent and problem-independent, so it can be generalized to more industrial applications, especially to the eCommerce industry which has similar data foundations and problems.",,,,,
IAAI-145,Preclinical Stage Alzheimer's Disease Detection Using Magnetic Resonance Image Scans,Fatih Altay|Guillermo Sánchez|Yanli James|Senem Velipasalar|Asif Salekin,"Alzheimer's disease is one of the diseases that mostly affects older people without being a part of aging. The most common symptoms include problems with communicating and abstract thinking, as well as disorientation. It is important to detect Alzheimer's disease in early stages so that cognitive functioning would be improved by medication and training. In this paper, we propose two attention model networks for detecting Alzheimer's disease from MRI images to help early detection efforts at the preclinical stage. We also compare the performance of these two attention network models with a baseline model. Recently available OASIS-3 Longitudinal Neuroimaging, Clinical, and Cognitive Dataset is used to train, evaluate and compare our models. The novelty of this research resides in the fact that we aim to detect Alzheimer's disease when all the parameters, physical assessments, and clinical data state that the patient is healthy and showing no symptoms.",,,,,
IAAI-149,EeLISA: Combating Global Warming Through the Rapid Analysis of Eelgrass Wasting Disease,Brendan Rappazzo|Morgan Eisenlord|Olivia Graham|Lillian Aoki|Phoebe Dawkins|Drew Harvell|Carla Gomes,"Global warming is the greatest threat facing our planet, and is causing environmental disturbance at an unprecedented scale. We are strongly positioned to leverage the advancements of Artificial Intelligence (AI) and Machine Learning (ML) which provide humanity, for the first time in history, an analysis and decision making tool at massive scale. Strong evidence supports that global warming is contributing to marine ecosystem decline, including eelgrass habitat. Eelgrass is affected by an opportunistic marine pathogen and infections are likely exacerbated by rising ocean temperatures. The necessary disease analysis required to inform conservation priorities is incredibly laborious, and acts as a significant bottleneck for research. To this end, we developed EeLISA (Eelgrass Lesion Image Segmentation Application). EeLISA enables ecologist experts to train a segmentation module to perform this crucial analysis at human level accuracy, while minimizing their labeling time and integrating into their existing workflow. EeLISA has been deployed for over 16 months, and has facilitated the preparation of four manuscripts including a critical eelgrass study ranging from Southern California to Alaska. These studies, utilizing EeLISA, have led to scientific insight and discovery in marine disease ecology.",,,,,
IAAI-151,Author Homepage Discovery in CiteSeerX,Krutarth Patel|Cornelia Caragea|Doina Caragea|C. Lee Giles,"Scholarly digital libraries provide access to scientific publications and comprise useful resources for researchers. CiteSeerX is one such digital library search engine that provides access to more than 10 million academic documents. We propose a novel search-driven approach to build and maintain a large collection of homepages that can be used as seed URLs in any DL including CiteSeerX to crawl scientific documents. Precisely, we integrate Web search and classification in a unified approach to discover new homepages: first, we use publicly-available author names and research paper titles as queries to a Web search engine to find relevant content, and then we identify the correct homepages from the search results using a powerful deep learning classifier based on Convolutional Neural Networks. Moreover, we use Self-Training in order to reduce the labeling effort and to utilize the unlabeled data to train the efficient researcher homepage classifier. Our experiments on a large scale dataset highlight the effectiveness of our search-driven approach, and position Web search as an effective method for acquiring authors' homepages. We show the development and deployment of the proposed approach in CiteSeerX and the maintenance requirements. Our datasets and code will be made available online.",,,,,
IAAI-190,Using Unsupervised Learning for Data-driven Procurement Demand Aggregation,Eran Shaham|Adam Westerski|Rajaraman Kanagasabai|Amudha Narayanan|Samuel Ong|Jiayu Wong|Manjeet Singh,"Procurement is an essential operation of every organization, regardless of its size or domain. As such, aggregating the demands could lead to a better value-for-money due to: (1) lower bulk prices; (2) larger vendor tendering; (3) lower shipping and handling fees; and (4) reduced legal and admin overhead. This paper describes our experience on developing an AI solution for demand aggregation and deploying it in A*STAR, a large governmental research organization in Singapore, with procurement expenditure in the scale of 100’s of millions of dollars yearly. We formulate the demand aggregation problem using a bipartite graph model depicting the relationship between procured items and target vendors, and show that identifying maximal edge bicliques within that graph would reveal potential demand aggregation patterns. We propose an unsupervised learning methodology for efficiently mining such bicliques using a novel Monte Carlo sub-space clustering approach. Based on this, a proof-of-concept prototype was developed and tested with the end-users during 2017, and later trialed and iteratively refined, before being rolled out in 2019. The final performance achieved on past cases benchmark was: 100% precision (all aggregation opportunities identified by the engine were correct) and 71% re-call (the engine correctly identified 71% of the past aggregation exercises that were transformed into bulk tenders). The performance for new opportunities pointed out by the engine was 81% (i.e., 81% of the newly identified cases were deemed useful cases for potential bulk tender contracts in the future). Overall, the cost savings from the true positive contracts spotted so far are estimated to be S$7M annually.",,,,,
IAAI-21,JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase,Wanying Ding|Vinay Chaudhri|Naren Chittar|Krihshna Konakanchi,"Knowledge Graphs have emerged as a compelling abstraction for capturing key relationship among the entities of interest to enterprises and for integrating data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by leveraging knowledge graphs across the organization for multiple mission critical applications such as risk assessment, fraud detection, investment advice, etc. A core problem in leveraging a knowledge graph is to link mentions (e.g., company names) that are encountered in multiple information sources to entities in the knowledge graph. Although several techniques exist for entity linking, but they are tuned for entities that exist in Wikipedia, and fail to generalize for the entities that are of interest to an enterprise. In this paper, we propose a novel end-to-end neural entity linking model (JEL) that deploys a margin loss to generate entity embedding with minimum context information, and implements a Wide & Deep Learning to match character and semantic information respectively. We show that JEL achieves the state-of-the-art performance to link mentions of company names in financial news with entities in our knowledge graph. We report on our efforts to deploy this model in the company-wide system to generate alerts in response to financial news. The methodology used for JEL is directly applicable and usable by other enterprises who need entity linking solutions for data that is unique to their respective situation.",,,,,
IAAI-34,VRU Pose-SSD:  Multiperson  Pose  Estimation  For  Automated  Driving,Chandan Kumar|Jayanth Ramesh|Bodhisattwa Chakraborty|Renjith Raman|Christoph Weinrich|Anurag Mundhada|Arjun Jai|Fabian Flohr,"We present a fast and efficient approach for joint person detection and pose estimation optimized for automated driving (AD) in urban scenarios. We use a multitask weight sharing architecture to jointly train detection and pose estimation. This modular architecture allows us to accommodate different downstream tasks in the future. By systematic large-scale experiments on the Tsinghua-Daimler Urban Pose Dataset (TDUP), we obtain multiple models with varying accuracy-speed trade-offs. We then quantize and optimize our network for deployment and present a detailed analysis of the efficacy of the algorithm. We introduce a two-stage evaluation strategy, which is more suitable for AD and achieve a significant performance improvement in comparison to state-of-the-art approaches. Our optimized model runs at 52~fps on full HD images and still reaches a competitive performance of 32.25~LAMR. We are confident that our work serves as an enabler to tackle higher-level tasks like VRU intention estimation and gesture recognition, which rely on stable pose estimates and will play a crucial role in future AD systems.",,,,,
IAAI-55,Predicting Accidents in the Mining Industry with a Multitask Learning Approach,Rodolfo Palma|Luis Martí|Nayat Sanchez-Pi,"The mining sector is a very relevant part of the  Chilean economy, representing more than 14\% of the country’s GDP and more than 50\% of its exports. However, mining is also a high-risk activity where health, safety, and environmental aspects are fundamental concerns to take into account to render it viable in the longer term. The Chilean National Geology and Mining Service (SERNAGEOMIN, after its name in Spanish) is in charge of ensuring the safe operation of mines. On-site inspections are their main tool in order to detect issues, propose corrective measures, and track the compliance of those measures.  Consequently, it is necessary to create inspection programs relying on a data-based decision-making strategy.        This paper reports the work carried out in one of the most relevant dimensions of said strategy: predicting the mining worksites accident risk. That is, how likely it is a mining worksite to have accidents in the future. This risk is then used to create a priority ranking that is used to devise the inspection program. Estimating this risk at the government regulator level is particularly challenging as there is a very limited and biased data.        Our main contribution is to apply a multi-task learning approach to train the risk prediction model in such a way that is able to overcome the constraints of the limited availability of data by fusing different sources. As part of this work, we also implemented a human-experience-based model that captures the procedures currently used by the current experts in charge of elaborating the inspection priority ranking.        The mining worksites risk rankings built by model achieve a 121.2% NDCG performance improvement over the rankings based on the currently used experts’ model and outperforms the non-multi-task learning alternatives.",,,,,
IAAI-59,Ontology-Enriched Query Answering on Relational Databases,Shqiponja Ahmetaj|Vasilis Efthymiou|Ronald Fagin|Phokion Kolaitis|Chuan Lei|Fatma Ozcan|Lucian Popa,"We develop a flexible, open-source framework for query answering on relational databases by adopting methods and techniques from the Semantic Web community and the data exchange community, and we apply this framework to a medical use case. We first deploy module-extraction techniques to derive a concise and relevant sub-ontology from an external reference ontology. We then use the chase procedure from the data exchange community to materialize a universal solution that can be subsequently used to answer queries on an enterprise medical database. Along the way, we identify a new class of well-behaved acyclic EL-ontologies extended with role hierarchies, suitably restricted functional roles, and domain/range restrictions, which cover our use case. We show that such ontologies are C-stratified, which implies that the chase procedure terminates in polynomial time. We provide a detailed overview of our real-life application in the medical domain and demonstrate the benefits of this approach, such as discovering additional answers and formulating new queries.",,,,,
IAAI-60,Finding Needles in Heterogeneous Haystacks,Bijaya Adhikari|Liangyue Li|Nikhil Rao|Karthik Subbian,"Due to intense competition and lack of real estate on the front page of large e-commerce platforms, sellers are sometimes motivated to garner non-genuine signals (clicks, add-to-carts, purchases) on their products, to make them appear more appealing to customers. This hurts customers' trust on the platform, and also hurts genuine sellers who sell their items without looking to game the system. While it is important to find the sellers and the buyers who are colluding to garner these non-genuine signals, doing so is highly nontrivial. Firstly, the set of bad actors in the system is a very small fraction of all the buyers/sellers on the platform. Secondly, bad actors ``hide"" with the good ones, making them hard to detect. In this paper, we develop CONGCN, a context aware heterogeneous graph convolutional network to detect bad actors on a large heterogeneous graph. While our method is motivated by abuse detection in e-commerce, the method is applicable to other areas such as computational biology and finance, where large heterogeneous graphs are pervasive, and the amount of labeled data is very limited. We train CONGCN via novel sampling methods, and context aware message passing in a semi-supervised fashion to predict dishonest buyers and sellers in e-commerce. Extensive experiments show that our method is effective, beating several baselines; generalizable to an inductive setting and highly scalable",,,,,
IAAI-74,Shape-based Feature Engineering for Solar Flare Prediction,Varad Deshmukh|Thomas Berger|James Meiss|Elizabeth Bradley,"Solar flares are caused by magnetic eruptions in activeregions (ARs) on the surface of the sun.  These events can havesignificant impacts on human activity, many of which can be mitigatedwith enough advance warning from good forecasts.  To date, machinelearning-based flare-prediction methods have employed physics-basedattributes of the AR images as features; more recently, there has beensome work that uses features deduced automatically by deep learningmethods (such as convolutional neural networks).  We describe a suiteof novel shape-based features extracted from magnetogram images of theSun using the tools of computational topology and computationalgeometry.  We evaluate these features in the context of a multi-layerperceptron (MLP) neural network and compare their performance againstthe traditional physics-based attributes.  We show that these abstractshape-based features outperform the features chosen by the humanexperts, and that a combination of the two feature sets improves theforecasting capability even further.",,,,,
IAAI-76,Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change,Taeyeong Choi|Benjamin Pyenson|Juergen Liebig|Theodore Pavlic,"Biology is both an important application area and a source of motivation for development of advanced machine learning techniques. Although much attention has been paid to large and complex data sets resulting from high-throughput sequencing, advances in high-quality video recording technology have begun to generate similarly rich data sets requiring sophisticated techniques from both computer vision and time-series analysis. Moreover, just as studying gene expression patterns in one organism can reveal general principles that apply to other organisms, the study of complex social interactions in an experimentally tractable model system, such as a laboratory ant colony, can provide general principles about the dynamics many other social groups. Here, we focus on one such example from the study of reproductive regulation in small laboratory colonies of ~50 Harpgenathos ants. These ants can be artificially induced to begin a ~20 day process of hierarchy reformation. Although the conclusion of this process is conspicuous to a human observer, it is still unclear which behaviors during the transients are contributing to the process. To address this issue, we explore the potential application of One-class Classification (OC) to the detection of abnormal states in ant colonies for which behavioral data is only available for the normal societal conditions during training. Specifically, we build upon the Deep Support Vector Data Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN) that synthesizes fake “inner outlier” observationsduring training that are near the center of the DSVDD data description.We show that IO-GEN increases the reliability of the final OC classifier relative to other DSVDD baselines. This method can be used to screen video frames for which additional human observation is needed. Although we focus on an application with laboratory colonies of social insects, this approach may be applied to video data from other social systems to either better understand the causal factors behind social phase transitions or even to predict the onset of future transitions.",,,,,
IAAI-77,"Where there's Smoke, there's Fire: Wildfire Risk Predictive Modeling via Historical Climate Data",Shahrzad Gholami|Narendran Kodandapani|Jane Wang|Juan M. Lavista Ferres,"Wildfire is a growing global crisis with devastating consequences. Uncontrolled wildfires take away human lives, destroy millions of animals and trees, degrade the air quality, impact the biodiversity of the planet and cause substantial economic costs.It is incredibly challenging to predict the spatio-temporal likelihood of wildfires based on historical data, due to their stochastic nature. Crucially though, the accurate and reliable prediction of wildfires can help the stakeholders and decision-makers take timely, strategic and effective actions to prevent, detect and suppress the wildfires before they become unmanageable. Unfortunately, most previous studies developed predictive models that suffer from some shortcomings: (i) they do not take the temporal aspects into account precisely and they assume the independent and identically distributed random variables in the evaluation phase; (ii) they do not evaluate their approaches comprehensively, thus it is not clear if their proposed predictions and selected models are reliable across different locations and time steps for practical deployment; and (iii) for the supervised learning models, they use predictor features and fire observations from the same time step in the training phase, which makes the inference task infeasible for future fire prediction. In this paper, we revisit the wildfire predictive modeling, explore the inherent challenges from a practical perspective and evaluate our modeling approach comprehensively via historical burned areas, climate and geospatial data from three vast landscapes in India.",,,,,
IAAI-87,Reinforcement Learning - based Product Delivery Frequency Control,Yang Liu|Zhengxing Chen|Kittipat Virochsiri|Juan Wang|Jiahao Wu|Feng Liang,"Frequency control is an important problem in modern recommender systems. It dictates the delivery frequency of recommendation to maintain product quality and efficiency. For example, the frequency of delivering campaign notifications impacts daily metrics as well as  infrastructure resource consumption. There remain open questions on what objective we should optimize for to best represent business values in long term, and how we should balance between daily metrics and resource consumption in a dynamically fluctuating environment. We propose a personalized methodology for the frequency control problem, which combines long-term value optimization using reinforcement learning (RL) with a robust volume control technique we termed 'Effective Factor'. We demonstrate statistically significant improvement in daily metrics and resource efficiency by our method in several notification applications at a scale of billions of users. To our best knowledge, our study represents the first deep RL application on the frequency control problem at such an industrial scale.",,,,,
IAAI-98,A Novel AI-based Methodology for Identifying Cyber Attacks in Honey Pots,Muhammed Abuodeh|Christian Adkins|Omid Setayeshfar|Prashant Doshi|Kyu Lee,"We present a novel AI-based methodology that identifies phases of a host-level cyber attack simply from system call logs. System calls emanating from cyber attacks on hosts such as honey pots are often recorded in audit logs. Our methodology first involves efficiently loading, caching, processing, and querying system events contained in audit logs in support of computer forensics. Output of queries remains at the system call level and is difficult to process. The next step is to infer a sequence of abstracted actions, which we colloquially call a storyline, from the system calls given as observations to a latent-state probabilistic model. These storylines are then accurately identified with class labels using a learned classifier. We qualitatively and quantitatively evaluate methods and models for each step of the methodology using 114 different attack phases collected by logging the attacks of a red team on a server, on some likely benign sequences containing regular user activities, and on traces from a recent DARPA project. The resulting end-to-end system, which we call Cyberian, identifies the attack phases with a high level of accuracy illustrating the benefit that this machine learning-based methodology brings to security forensics.",,,,,
IAAI-99,Carbon to Diamond: An Incident Remediation Assistant System From SiteReliability Engineers’ Conversations in Hybrid Cloud Operations,Suranjana Samanta|Ajay Gupta|Prateeti Mohapatra|Amar Prakash Azad,"Conversational channels are changing the landscape of hybrid cloud service management. These channels are becoming important avenues for Site Reliability Engineers (SREs) %Subject Matter Experts (SME) to collaboratively work together to resolve an incident or issue. Identifying segmented conversations and extracting key insights or artefacts from them can help engineers to improve the efficiency of the incident remediation process by using information retrieval mechanisms for similar  incidents.  However, it has been empirically observed that due to the semi-formal behavior of such conversations (human language) the conversations are very unique in nature and also contain domain-specific terms. %It is important to identify the correct keywords and artefacts like symptoms, issue etc., present in the conversation chats. In this paper, we build a framework that taps into the conversational channels and uses various learning methods to (1) understand and extract key artefacts from conversations like diagnostic steps and resolution actions taken and (2) present an approach to identify past conversations about similar issues. Experimental results on our dataset show the efficacy of the methods used in our proposed system.",,,,,
IAAI-101,Predicting Parking Availability from Mobile Payment Transactions with Positive Unlabeled Learning,Jonas Sonntag|Michael Engel|Lars Schmidt-Thieme,"Cruising for parking is a problem for many motorists and for communities that need to reduce emissions. A widespread provision of parking assistance to address this problem requires a scalable system to generate availability information. Existing approaches to estimate parking availability at scale use supervised learning and hence depend on ground-truth labels e.g. from sensors, user surveys or manual data collection.This dependency restricts the widespread deployment and sustainable operation especially the monitoring and re-training processes of the respective models.We describe a parking availability prediction system that resolves the dependency on customary ground-truth labeling. The new approach is based solely on data from parking ticket bookings via a mobile phone app. Every parking transaction serves as an implicit signal for parking availability which we leverage by applying algorithms and metrics for positive-unlabeled learning (PU-learning). This approach enables the deployment in diverse regions, as well as the scalable monitoring and retraining of models. We evaluate our framework on a real-world dataset in Seattle.",,,,,
IAAI-104,Personalizing Individual Comfort in the Group Setting,Emil Laftchiev|Diego Romeres|Daniel Nikovski,"Maintaining individual thermal comfort in indoor spaces shared by multiple occupants is difficult because it requires both intuition about the thermal properties of the room, as well as an understanding of the thermal comfort preferences of each individual. We explore an approach to optimizing individual thermal comfort within a group through temperature set-point optimization of HVAC equipment. We propose a weakly-supervised algorithm to learn the individual thermal comfort preferences and an autoencoding framework to learn static approximations of room thermodynamics. We further propose two approaches to learn a control law that sets the HVAC set-points subject to the preferred user temperatures. The proposed method is tested on a real data-set obtained from workers in an open office. The results show that, on average, the temperature in the room at each user's location can be regulated to within 0.5C of the user's desired temperature.",,,,,
IAAI-105,Device Fabrication Knowledge Extraction from Materials Science Literature,Sapan Shah|Neelanshi Wadhwa|Sarath S|Sreedhar Reddy|Pritwish Mitra|Deepak Jain|Beena Rai,"Devices like solar cells, batteries etc. often comprise of a host of material types including organic, inorganic and hybrid materials. The fabrication procedures for these devices involve screening or designing the right set of materials and then subjecting them to a sequence of operations under very specific conditions. The performance characteristics of a device critically depend on the materials used in its fabrication, the specific operations carried out, their operating conditions and the specific sequence in which they are carried out. The space of potential materials, operations and operating conditions is vast, and selecting the right combination thereof to achieve the desired characteristics is a knowledge intensive activity. A large amount of such device fabrication knowledge is available in the form of publications, patents, company reports and so on. In this paper, we present a system that systematically extracts this knowledge from materials science literature. The extracted knowledge is represented as knowledge graphs conforming to an ontology that can be queried to make informed decisions in device fabrication procedures. The system first identifies the set of relevant paragraphs that contain fabrication knowledge. It then employs state of the art entity and relation extraction models to identify instances of operations, methods, materials, etc. and relations between them. The system then applies an unsupervised algorithm to identify sequences of operations representing fabrication procedures. We applied our system on solar cell fabrication knowledge extraction and achieved good performance. We believe our results provide much needed impetus for further work in this area.",,,,,
IAAI-113,Using Online Planning and Acting to Recover from Cyberattacks on Software-defined Networks,Sunandita Patra|Alex Velazquez|Myong Kang|Dana Nau,"We describe ACR-SDN, a system to monitor, diagnose, andquickly respond to attacks or failures that may occur insoftware-defined networks (SDNs). An integral part of ACRSDNis its use of RAE+UPOM, an automated acting andplanning engine that uses hierarchical refinement. To adviseACR-SDN on how to recover a target system from faults andattacks, RAE+UPOM uses attack recovery procedures writtenas hierarchical operational models. Our experimental resultsshow that the use of refinement planning in ACR-SDNis successful in recovering SDNs from attacks with respect tothree performance metrics: estimated time for recovery, efficiency,and, retry ratio.",,,,,
IAAI-119,SKATE: A Natural Language Interface for Encoding Structured Knowledge,Clifton McFate|Aditya Kalyanpur|Dave Ferrucci|Andrea Bradshaw|Ariel Diertani|David Melville|Lori Moon,"In Natural Language (NL) applications, there is often a mismatch between what the NL interface is capable of interpreting and what a lay user knows how to express. This work describes a novel natural language interface that reduces this mismatch by refining natural language input through successive, automatically generated semi-structured templates. In this paper we describe how our approach, called SKATE, uses a neural semantic parser to parse NL input and suggest semi-structured templates, which are recursively filled to produce fully structured interpretations. We also show how SKATE integrates with a neural rule-generation model to interactively suggest and acquire commonsense knowledge. We provide a preliminary coverage analysis of SKATE for the task of story understanding, and then describe a current business use-case of the technology in a restricted domain: COVID-19 policy design.",,,,,
IAAI-142,Data-driven Multi-mode Patrol Planning for Anti-poaching,Weizhe Chen|Weinan Zhang|Duo Liu|Weiping Li|Xiaojun Shi|Fei Fang,"Wildlife poaching is threatening key species that play importantroles in the ecosystem. With historical ranger patrolrecords, it is possible to provide data-driven predictions ofpoaching threats and plan patrols to combat poaching. However,the patrollers often patrol in a multimodal way, whichcombines driving and walking. It is a tedious task for thedomain experts to manually plan such a patrol, and the resultoften falls into sketchy routes. In this paper, we proposea way of doing data-driven multimodal patrol planning.We used machine learning models to predict the threats andthen use a novel mixed-integer linear programming to planthe route. With a field deployment focus on the machinelearning prediction result at Jilin Huangnihe National NatureReserve (HNHR) in Northeast China in December 2019, therangers found 42 snares, which is significantly higher thanthe historical record. We also use offline experiments to showthat our route planning method can improve the efficiency ofpatrol and can potentially serve as the basis of future deployment.",,,,,
IAAI-146,Path to Automating Ocean Health Monitoring,Mak Ahmad|Scott Penberthy|Abigail Powell,"Marine ecosystems directly and indirectly impact human health, providing benefits such as essential food sources, coastal protection and biomedical compounds. Monitoring changes in marine species is important because impacts such as overfishing, ocean acidification and hypoxic zones can negatively affect both human and ocean health. The US west coast supports a diverse assemblage of deep-sea corals that provide habitats for fish and numerous other invertebrates. Currently, National Oceanic Atmospheric Administration (NOAA) scientists manually track the health of coral species using extractive methods. In this paper, we test the viability of using a machine learning algorithm Convolutional Neural Network (CNN) to automatically classify coral species, using field-collected coral images in collaboration with NOAA. We fine tune the hyperparameters of our model to surpass the human F-score. We also highlight a scalable opportunity to monitor ocean health automatically while preserving corals.",,,,,
IAAI-152,DeepCOVID: An Operational DL-driven Framework for Explainable Real-time COVID-19 Forecasting,Alexander Rodriguez|Anika Tabassum|Jiaming Cui|Jiajia Xie|Javen Ho|Pulak Agarwal|Bijaya Adhikari|B. Aditya Prakash,"How do we forecast emerging pandemic in real time in a purely data-driven manner? How to leverage rich heterogeneous data based on various signals such as mobility, testing, and/or disease exposure for forecasting? How do we handle noisy data and generate uncertainties in the forecast? In this paper, we present DeepCOVID, a novel deep learning framework designed for real-time Covid-19 forecasting. DeepCOVID works well with sparse data and can handle noisy heterogeneous data signals by propagating the uncertainty from the data in a principled manner resulting in meaningful uncertainties in the forecast. The framework also consists of modules for both real-time and retrospective exploratory analysis to enable interpretation of the forecast. Results from real-time predictions (featured on the CDC website and FiveThirtyEight.com) since April 2020 indicates that our approach is competitive among the methods in the COVID-19 Forecast Hub, especially for short-term predictions.",,,,,
IAAI-154,Deep Epidemiological Modeling by Black-box Knowledge Distillation: An Accurate Deep Learning Model for COVID-19,Dongdong Wang|Liqiang Wang|Shunpu Zhang,"Precise prediction on epidemiological transmission dynamics is crucial to the prevention of infectious diseases in public health such as COVID-19. The quality of forecasting is subject to the model accuracy for complex transmission dynamics under varying conditions. However, it is almost infeasible to design a single physical model to capture all complicated dynamics in epidemiological transmission. A mixture model that integrates many epidemiological models is a potential solution; however, it is very difficult to build an accurate yet efficient mixture model using conventional approaches. To address this issue, we propose a novel deep learning approach using black-box knowledge distillation for practical transmission dynamics prediction. First, we develop a simulation system consisting of many mixture models with different parameters. Next, we use observation sequences to query the simulation system to achieve projection sequences as knowledge. Then, we train a student deep neural network with the acquired observation-projection sequence pairs. To improve efficiency of query, sequence mixup is proposed to reduce model queries and remain distillation accuracy. The case study on COVID-19 shows this black-box knowledge distillation approach works very accurately and efficiently, ie, achieves precise projection results with much lower computation cost in realistic complex scenarios.",,,,,
IAAI-157,Twitter Event Summarization by Exploiting Semantic Terms and Graph Network,Quanzhi Li|Qiong Zhang,"Twitter is a fast communication channel for gathering and spreading breaking news, and it generates a large volume of tweets for most events. Automatically creating a summary for an event is necessary and important. In this study, we explored two extractive approaches for summarizing events on Twitter. The first one exploits the semantic types of event related terms, and ranks the tweets based on the score computed from these semantic terms. The second one utilizes a graph convolutional network built from a tweet relation graph to generate tweet hidden features for tweet salience estimation. And the most salient tweets are selected as the summary of the event.   Our experiments on 1,000 events show that these two approaches outperform the compared methods.",,,,,
IAAI-165,Spatiotemporal Graph Neural Network for Performance Prediction of Photovoltaic Power Systems,Ahmad Maroof Karimi|Yinghui Wu|Mehmet Koyuturk|Roger French,"Accurate forecasting of photovoltaic (PV) performance measuresis critical for the reliability of PV systems. In recentyears, a large number of PV systems have been added to theelectrical grid as well as installed as off-grid systems. Thetrend suggests deployment of PV system will continue to risein the future. Due to complex non-linear variability in poweroutput from the PV systems, forecasting PV power is a nontrivialtask. The negative impact of the variability affects thestability and planning of a power system network. For thisreason accurate forecasting of PV system performance canreduce the uncertainty caused by variability. In this work,we leverage spatial and temporal coherence among the powerplants for PV power forecasting. Our approach is motivatedby the observation that power plants in a region undergo similarexposure from the environment and thus behavior of onepower plant can help forecast the values of other plants. Tomodel the relationship between PV plants, we used a spatiotemporalgraph neural network (st-GNN) and train machinelearning models to forecast the PV performances. Inour graph model, nodes represent PV systems and edges representthe proximity between PV systems. Our computationalexperiments on large-scale data from a network of 316 systemsshow that spatiotemporal forecasting of PV power performssignificantly better than a model that only applies temporalconvolution to isolated nodes. Furthermore, the longerthe future forecast time the difference between the performanceof the spatiotemporal forecasting and isolated networkwhen only temporal convolution is applied increases further.",,,,,
IAAI-167,Structural Attention Mechanism and Automated Semantic Segmentation Ensembled for Uncertainty Prediction,Charles Kantor|Brice Rauby|Léonard Boussioux|Emmanuel Jehanno|Hugues Talbot,"While the basic methods of image classification are bench-marked on large databases of widely varying objects, many AI real-world applications require advanced, fine-grained classification, to distinguish between items with similar global patterns, like insects or birds, but that differ by small details.  Therefore, we propose in this paper to use attention and segmentation methods to distinguish foreground from background, and to use this as a confidence measure, based on the overlap between the segmentation and the attention masks. We show that confidence in the classification grows as this overlap increases. This confidence and identification tools are of practical interest in biology for automated wildlife recognition and we focus on the butterfly classification use case as a useful proof-of-concept for entomologists. Our technology is currently deployed in real-world on widely used crowdsourcing platforms and museums to annotate large scale data efficiently and engage citizen scientists.",,,,,
IAAI-171,Deepening the Sense of Touch in Planetary Exploration with Geometric and Topological Deep Learning,Yuzhou Chen|Yuliya Marchetti|Yulia R. Gel,"Tactile and embedded sensing is a new concept that has recently appeared in the context of rovers and planetary exploration missions. Various sensors such as those measuring pressure and integrated directly on wheels have the potential to add a ""sense of touch"" to exploratory vehicles. We investigate the utility of deep learning (DL), from conventional Convolutional Neural Networks (CNN) to emerging geometric and topological DL, to terrain classification for planetary exploration based on a novel dataset from an experimental tactile wheel concept. The dataset includes 2D conductivity images from a pressure sensor array, which is wrapped around a rover wheel and is able to read pressure signatures of the ground beneath the wheel. Neither newer nor traditional DL tools have been previously applied to tactile sensing data. We discuss insights into advantages and limitations of these methods for the analysis of non-traditional pressure images and their potential use in planetary surface science.",,,,,
IAAI-174,A Reciprocal Embedding Framework For Modelling Mutual Preferences,Ramanathan R|Nicolas K. Shinada|Michinobu Shimatani|Yuhei Yamaguchi|Junichi Tanaka|Yuta Iizuka|Sucheendra K. Palaniappan,"Understanding the mutual preferences between potential dating partners is core to the success of modern web-scale personalized recommendation systems that power online dating platforms. In contrast to classical user-item recommendation systems which model the unidirectional preferences of users to items, understanding the bidirectional preferences between people in a reciprocal recommendation system (RRS) is more complex and challenging given the dynamic nature of interactions. In this paper, we describe a reciprocal recommender system we built for one of the leading online dating applications in Japan. We also discuss the lessons learnt from designing, developing and deploying the RRS in production.",,,,,
IAAI-176,Topological machine learning methods for power system responses to contingencies,Brian Bush|Yuzhou Chen|Dorcas Ofori-Boateng|Yulia R. Gel,"While deep learning tools, coupled with the emerging machinery of topological data analysis, are proven to deliver various performance gains in a broad range of applications, from image classification to biosurveillance to blockchain fraud detection, their utility in areas of high societal importance such as power system modeling and, particularly, resilience quantification in the energy sector yet remain untapped. To provide fast acting synthetic regulation and contingency reserve services to the grid while having minimal disruptions on customer quality of service, we propose a new topology-based system that depends on neural network architecture for impact metrics classification and prediction in power systems. This novel topology-based system allows one to evaluate the impact of three power system contingency types, namely, in conjunction with transmission lines, transformers, and transmission lines combined with transformers. We show that the proposed new neural network architecture equipped with local topological measures facilitates both more accurate classification of unserved load as well as the amount of unserved load. In addition, we are able to learn complex relationships between electrical properties and local topological measurements on the simulated response to contingencies for NREL-SIIP power system.",,,,,
IAAI-179,Attr2Style: A Transfer Learning Approach for Inferring Fashion Styles via Apparel Attributes,Rajdeep Hazra Banerjee|Abhinav Ravi|Ujjal Kr Dutta,"Popular fashion e-commerce platforms mostly provide de- tails about low-level attributes of an apparel (for example, neck type, dress length, collar type, print etc) on their prod- uct detail pages. However, customers usually prefer to buy apparels based on their style information, or simply put, occa- sion (for example, party wear, sports wear, casual wear etc). Application of a supervised image-captioning model to gen- erate style-based image captions is limited because obtaining ground-truth annotations in the form of style-based captions is difficult. This is because annotating style-based captions re- quires a certain amount of fashion domain expertise, and also adds to the costs and manual effort. On the contrary, low-level attribute based annotations are much more easily available. To address this issue, we propose a transfer-learning based image captioning model that is trained on a source dataset with sufficient attribute-based ground-truth captions, and used to predict style-based captions on a target dataset. The target dataset has only a limited amount of images with style-based ground-truth captions. The main motivation of our approach comes from the fact that most often there are correlations among the low-level attributes and the higher-level styles for an apparel. We leverage this fact and train our model in an encoder-decoder based framework using attention mechanism. In particular, the encoder of the model is first trained on the source dataset to obtain latent representations capturing the low-level attributes. The trained model is fine-tuned to gen- erate style-based captions for the target dataset. To highlight the effectiveness of our method, we qualitatively demonstrate that the captions generated by our approach are close to the actual style information for the evaluated apparels. Our model is deployed at Myntra (www.myntra.com)",,,,,
IAAI-17,HetSeq: Distributed GPU Training on Heterogeneous Infrastructure,Yifan Ding|Nicholas Botzer|Tim Weninger,"Modern deep learning systems like PyTorch and Tensorflow are able to train enormous models with billions (or trillions) of parameters on a distributed infrastructure. These systems require that the internal nodes have the same memory capacity and compute performance. Unfortunately, most organizations, especially universities, have a piecemeal approach to purchasing computer systems resulting in a heterogeneous infrastructure, which cannot be used to compute large models. The present work describes HetSeq, a software package adapted from the popular PyTorch package that provides the capability to train large neural network models on heterogeneous infrastructure. Experiments with transformer translation and BERT language model shows that HetSeq scales over heterogeneous systems. HetSeq can be easily extended to other models like image classification. Package with supported document is publicly available at https://github.com/yifding/hetseq.",,,,,
IAAI-37,Representing the Unification of Text Featurization using a Context-Free Grammar,Doruk Kilitcioglu|Serdar Kadioglu,"We propose a novel context-free grammar to represent text embeddings in conjunction with various transformations applied to those. We show how this grammar can serve as a unification layer on top of different featurization techniques, and their hybridization thereof. The approach is embodied in an open-source library, called TextWiser, with a high-level user interface to serve researchers and practitioners. The goal of TextWiser aims to enable rapid experimentation with various featurization methods and serve as a building block within AI applications consuming unstructured data. We highlight several key benefits that are desirable especially in industrial settings where rapid experimentation, reusability, reproducibility, and time to market are of great interest. Finally, we showcase a deployed service powered by TextWiser as an example of a successful enterprise application.",,,,,
IAAI-31,Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,Sean McGregor,"Mature industrial sectors (e.g., aviation) collect their real world failures in incident databases to inform safety improvements. Intelligent systems currently cause real world harms without a collective memory of their failings. As a result, companies repeatedly make the same mistakes in the design, development, and deployment of intelligent systems. A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society.The AI Incident Database is an incident collection initiated by an industrial/non-profit cooperative to enable AI incident avoidance and mitigation. The database supports a variety of research and development use cases with faceted and full textsearch on more than 1,000 incident reports archived to date.",,,,,
IAAI-43,Combining Machine Learning and Human Experts to Predict Match Outcomes in Football: A Baseline Model,Ryan Beal|Stuart E. Middleton|Timothy Norman|Sarvapali Ramchurn,"In this paper, we present a new application-focused benchmark datasetand results from a set of baseline Natural Language Processing and Machine Learningmodels for prediction of match outcomes for games of football (soccer). By doing so wegive a baseline for the prediction accuracy that can be achieved exploiting both statisticalmatch data and contextual articles from human sports journalists. Our dataset is focuseson a representative time-period over 6 seasons of the English Premier League, and includesnewspaper match previews from The Guardian. The models presented in this paperachieve an accuracy of 63.18% showing a 6.9% boost on the traditional statistical methods.",,,,,
IAAI-65,Empirical Best Practices On Using Product-Specific Schema.org,Mayank Kejriwal|Ravi Kiran Selvam|Chien-Chun Ni|Nicolas Torzec,"Schema.org has experienced high growth in recent years. Structured descriptions of products embedded in HTML pages are now not uncommon, especially on e-commerce websites. The Web Data Commons (WDC) project has extracted schema.org data at scale from webpages in the Common Crawl and made it available as an RDF `knowledge graph' at scale. The portion of this data that specifically describes products offers a golden opportunity for researchers and small companies to leverage it for analytics and downstream applications. Yet, because of the broad and expansive scope of  this data, it is not evident whether the data is usable in its raw form. In this paper, we do a detailed empirical study on the product-specific schema.org data made available by WDC. Rather than simple analysis, the goal of our study is to devise an empirically grounded set of best practices for using and consuming WDC product-specific schema.org data. Our studies reveal five best practices, each of which is justified by experimental data and analysis.",,,,,
